{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f72679-d1d8-4dbe-90e6-df28aa84fd99",
   "metadata": {},
   "source": [
    "# The Multiple Testing Problem\n",
    "So far, we have managed to define a framework for testing hypotheses using contrasts. We have seen how to turn these contrasts into maps of $t$-statistics, and have seen how we can then threshold these maps using the associated $p$-values using a threshold of $p < 0.05$. Although it may seem like we are essentially done, those of you with some statistical knowledge may have already spotted a big issue known as the *multiple testing problem*.\n",
    "\n",
    "## The General Multiple Testing Problem\n",
    "To understand the multiple testing problem, consider what a significance threshold of 5% actually means. In the world of frequentist inference, if we repeated our experiment multiple times and performed the same hypothesis test, we would only expect a significant $p$-value to occur 5% of the time if the null hypothesis was true. A small $p$-value therefore means either something rare has happened, or the null hypothesis is false.\n",
    "\n",
    "This is fine for a *single* hypothesis test, but if we did keep repeating the experiment over and over again, then suddenly those rare events are not so rare anymore. To see this, consider that the probability of at least one result from all those repeated experiments being significant is given by\n",
    "\n",
    "$$\n",
    "\\text{FWER} = 1 - (1 - \\alpha)^{m}.\n",
    "$$\n",
    "\n",
    "This is known as the *family-wise error rate*, where $\\alpha$ is the significance level and $m$ is the number of tests. If we only conduct 1 test with $\\alpha = 0.05$, then \n",
    "\n",
    "$$\n",
    "\\text{FWER} = 1 - (1 - 0.05)^{1} = 1 - 0.95 = 0.05,\n",
    "$$\n",
    "\n",
    "as we would expect. However, if we conducted 10 tests then we get\n",
    "\n",
    "$$\n",
    "\\text{FWER} = 1 - (1 - 0.05)^{10} = 1 - 0.60 = 0.40.\n",
    "$$\n",
    "\n",
    "So suddenly, the chance of at least one of those tests being significant is 40%, not the 5% that we wanted. Remember, this is the probability of significance *when the null is true*. So even if there is no effect, there is suddenly a 40% chance of declaring a result significant and thus making a [Type I error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors).\n",
    "\n",
    "````{admonition} Advanced: Understanding the Probability of Multiple Testing\n",
    ":class: dropdown\n",
    "\n",
    "To understand the probability behind the multiple testing problem, imagine rolling multiple dice simultaneously. The probability of rolling a 6 with a single die is $1/6 \\approx 16.7\\%$. The probability of at least one die showing a 6 when rolling multiple dice is given by\n",
    "\n",
    "$$\n",
    "P(6) = 1 - (1-0.167)^{m}, \n",
    "$$\n",
    "\n",
    "where $m$ is the number of dice. If we were to roll 2 dice, this gives\n",
    "\n",
    "$$\n",
    "P(6) = 1 - (1-0.167)^{2} = 1 - 0.69 = 0.31, \n",
    "$$\n",
    "\n",
    "meaning our chance of getting a 6 had increased from $\\approx 16.7\\%$ to $\\approx 31\\%$. We can see this visually in {numref}`dice-fig`. Here, all the 36 possible combinations of outcomes across 2 dice are shown. If you count the number of outcomes where one of the dice shows a 6, you get $11/36 \\approx 0.31$. So we can see that there is a $31\\%$ chance of getting a 6 now.\n",
    "\n",
    "```{figure} images/dice.png\n",
    "---\n",
    "width: 800px\n",
    "name: dice-fig\n",
    "---\n",
    "Visualisation of all the possible outcomes from rolling two dice.\n",
    "```\n",
    "\n",
    "If we increase the number of dice to 10, we then get\n",
    "\n",
    "$$\n",
    "P(6) = 1 - (1-0.167)^{10} = 1 - 0.16 = 0.84, \n",
    "$$\n",
    "\n",
    "so now there is an $84\\%$ chance of getting a 6. If we keep increasing the number of dice, eventually we hit a point where we are basically guaranteed to see at least one 6 every time we roll. Suddenly, the rare(ish) event of a 6 is suddenly not rare at all.\n",
    "\n",
    "To connect this with the multiple testing problem, every time we repeat an experiment and then test the null hypothesis, we are rolling the dice again. If we then look across all the tests we have performed, we have to understand that the probability of seeing at least one significant $p$-value goes up and up until we hit a point where it is all but guaranteed to find something significant, *even if the null hypothesis is true*.\n",
    "\n",
    "````\n",
    "\n",
    "## The Multiple Testing Problem in Imaging\n",
    "The problem we have in imaging is that the hypothetical situation of repeating the same experiment again and again is exactly what we have done by fitting the same model at each voxel. When looking across an entire image, we are seeing our experiment repeated perhaps 100,000 times. So what are the chances that at least one voxel will be significant at the traditional 5% level? With 100,000 tests we have\n",
    "\n",
    "$$\n",
    "1 - (1 - 0.05)^{100000} = 1 - 0 = 1.\n",
    "$$\n",
    "\n",
    "So we have a 100% guarantee that we will see significant results, *even if the null hypothesis is true everywhere in the brain*.\n",
    "\n",
    "From this it should be clear that we have a problem, but what does that mean practically in terms of the number of false positive results? In the worst case scenario, this could mean 5% of the voxels are significant when the null is true. For our example image of 100,000 voxels, this could mean 5,000 false-positives in an image. This is not a small number of mistakes to make. \n",
    "\n",
    "This situation is illustrated in {numref}`poldrack-uncorr-thresh-fig` from [Poldrack, Mumford and Nichols (2011)](https://www-cambridge-org.manchester.idm.oclc.org/core/books/handbook-of-functional-mri-data-analysis/8EDF966C65811FCCC306F7C916228529#). The voxels within the yellow circle represent places where there is a real effect, and the voxels outside the yellow circle represent places with no effect. When thresholding using their example of 10% ($\\alpha = 0.10$), on average around 10% of the voxels with no effect will survive the threshold and thus indicate a significant result.\n",
    "\n",
    "```{figure} images/poldrack-uncorr-thresh.png\n",
    "---\n",
    "width: 800px\n",
    "name: poldrack-uncorr-thresh-fig\n",
    "---\n",
    "Visualisation of thresholding an image that contains true activations (within the circle) and noise (outside the circle) using $p < 0.10$. On average, across repeats, 10% of the noise voxels will be significant when using this threshold.\n",
    "```\n",
    "\n",
    "In this example, we can obviously see which results are *real* and which are *false*, but in a real image we would have no idea. Notice as well that the false positives can cluster together, thanks in part to the smoothing, and thus can give the impression of true regional activations. This is the insidious nature of false positives. So, overall, this is a pretty serious problem to have. With this in mind, it is not surprising that so much effort over the years has gone into developing correction techniques to try and get around this problem. These ''solutions'' will be the focus of the next section.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MATLAB Kernel",
   "language": "matlab",
   "name": "jupyter_matlab_kernel"
  },
  "language_info": {
   "file_extension": ".m",
   "mimetype": "text/x-matlab",
   "name": "matlab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}